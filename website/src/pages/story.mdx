---
layout: ../layouts/TextLayout.astro
title: The story
description: 
---
import '../styles/team.css';

In a world where humanity must rebuild from the ashes, games may seem like a trivial pursuit. Yet, in the aftermath of the apocalypse, our mission at the **Applied Digital Anthropology (ADA)** department of the New Age government is anything but trivial. Tasked with restoring the essence of human civilization, we’ve turned to an unexpected cornerstone of human experience: games.

Our team is exploring how machines can simulate human cognition and strategies to help shape the future of this planet. Why games? Because they encapsulate problem-solving, strategic thinking, and creativity — traits essential for humanity’s survival and progress. To this end, we’ve embarked on a case study of [**Wikispeedia**](https://dlab.epfl.ch/wikispeedia/play/), a game that challenges players to navigate Wikipedia’s web of articles from one topic to another using only hyperlinks. Simple, yet deeply revealing of how humans connect concepts, prioritize paths, and think under constraints.

## Meet the Team
<div class="team-container">
    <div class="team-card" style={{ width: '300px' }}>
        <img src="https://via.placeholder.com/150" alt="Dr. A. I." />
        <h3>Dr. A. I.</h3>
        <p>Our lead researcher, a brilliant mind in the field of artificial intelligence and human cognition.</p>
    </div>
    <div class="team-card" style={{ width: '300px' }}>
        <img src="https://via.placeholder.com/150" alt="Dr. E. M." />
        <h3>Dr. E. M.</h3>
        <p>Our resident expert in game theory and human psychology.</p>
    </div>
    <div class="team-card" style={{ width: '300px' }}>
        <img src="https://via.placeholder.com/150" alt="Dr. L. O." />
        <h3>Dr. L. O.</h3>
        <p>Our data scientist, who wrangles the vast amounts of data we collect from Wikispeedia.</p>
    </div>
    <div class="team-card" style={{ width: '300px' }}>
        <img src="https://pmd.lt/pp.jpg" />
        <h3>Dr. P. M.</h3>
        <p>Our game designer, who crafts the challenges that players face in Wikispeedia.</p>
    </div>
    <div class="team-card" style={{ width: '300px' }}>
        <img src="https://via.placeholder.com/150" alt="Dr. S. T." />
        <h3>Dr. S. T.</h3>
        <p>Our anthropologist, who studies the social dynamics of Wikispeedia players.</p>
    </div>
</div>

Together, we form **ADAspeedia**, a team united in exploring how machines can emulate and evolve human behavior. To discover the team members' individual stories, click [here](/team).

# A Human Blueprint
To recreate human-like behavior, we first need to understand how humans played Wikispeedia. What strategies did they use? Did they wander aimlessly or systematically? How did they balance the exploration of diverse topics with the focused goal of reaching their destination? These questions guided our initial analysis of the human dataset, revealing the intricacies of the paths taken, the cognitive shortcuts employed, and the common hurdles encountered.

For this, we have data extracted from the Previous Age. A team of researchers from EPFL has collected thousands of paths taken by humans, [unified in a single dataset](https://snap.stanford.edu/data/wikispeedia.html).  
We've analyzed it to keep only the essentials :
- **Paths** - The sequence of articles visited by players (id & title of the articles separated by a comma).


# The machine's understanding of the human language

Prior to our study, other methods already existed to compute the semantic distance between two words. Indeed, language models usually use a system of *word embeddings*, which is a specific representation of words and notions. An interesting point with these embeddings is that they can be visualized as high-dimensionnal vectors. They can hence be compared together with simpler comparison tools, such as cosine similarity and euclidean_distance.
Previous studies show that the embeddings learned by models usually capture the semantic behind words. For example, as embeddings are vectors, we can add them up together. It turns out that the result of the operation 'Greece' - 'Athens' + 'Paris' yields a very similar vector to the one representing 'France'.
We are now interested in the similarities in distribution between the difference in distances computed on paths played by humans VS our machine, and the difference in distances computed on paths played by humans VS existing human embeddings.

We first used a Bidirectional Encoder Representations from Transformers (BERT) base model. This architecture has proven to be consistantly powerful on multiple natural language processing tasks, specifically those related to semantic understanding. Hence, it seems like the perfect fit for our model.
To complement this, we also added a Sentence BERT (S-BERT) model. This architecture relies on the previous one but unlike it, it works on the semantic at sentence level instead of word level. As we may have multiple words in a Wikipedia article name, S-BERT might be able to capture more of the semantic similarities.

To study the distribution of differences, we followed the following protocol:

- Compute the distances for the paths played by humans and by our machine.

- Compute the difference of distance for each path in common.

- Extact the pair of articles.

- Compute for each of the pairs the BERT of each article.

- Based on the embeddings, compute the cosine similarity and euclidean distance.

- At the same time, compute the S-BERT cosine similarity for each pair of articles.

- For each of the BERT cosine similarity, BERT euclidean distance and S-BERT cosine similarity, compute their difference with the distances for the paths played by humans

Here are our results

