---
layout: ../layouts/TextLayout.astro
title: The story
description: 
---
import '../styles/team.css';
import { Alert, AlertTitle, AlertDescription } from "@/components/ui/alert";
import { Button } from "@/components/ui/button";

   <Alert variant="success">
        <AlertTitle>Si vous voulez faire une rÃ©fÃ©rence</AlertTitle>
        <AlertDescription>Utilisez ce component, qui permet de rediriger par ex. vers un outil du site</AlertDescription><br />
        <Button>ðŸ”Ž Explore Paths</Button>
    </Alert>

In a world where humanity must rebuild from the ashes, games may seem like a trivial pursuit. Yet, in the aftermath of the apocalypse, our mission at the **Applied Digital Anthropology (ADA)** department of the New Age government is anything but trivial. Tasked with restoring the essence of human civilization, weâ€™ve turned to an unexpected cornerstone of human experience: games.

Our team is exploring how machines can simulate human cognition and strategies to help shape the future of this planet. Why games? Because they encapsulate problem-solving, strategic thinking, and creativity â€” traits essential for humanityâ€™s survival and progress. To this end, weâ€™ve embarked on a case study of [**Wikispeedia**](https://dlab.epfl.ch/wikispeedia/play/), a game that challenges players to navigate Wikipediaâ€™s web of articles from one topic to another using only hyperlinks. Simple, yet deeply revealing of how humans connect concepts, prioritize paths, and think under constraints.

## Meet the Team
<div class="team-container">
    <div class="team-card" style={{ width: '300px' }}>
        <img src="/pp/ae.webp" alt="Dr. A. E." />
        <h3>Dr. A. E.</h3>
        <p>Our lead researcher, a brilliant mind in the field of artificial intelligence and human cognition.</p>
    </div>
    <div class="team-card" style={{ width: '300px' }}>
        <img src="/pp/tk.webp" alt="Dr. T. K." />
        <h3>Dr. T. K.</h3>
        <p>Our resident expert in game theory and human psychology.</p>
    </div>
    <div class="team-card" style={{ width: '300px' }}>
        <img src="/pp/gt.webp" alt="Dr. G. T." />
        <h3>Dr. G. T.</h3>
        <p>Our data scientist, who wrangles the vast amounts of data we collect from Wikispeedia.</p>
    </div>
    <div class="team-card" style={{ width: '300px' }}>
        <img src="/pp/pm.webp" alt="Dr. P. M." />
        <h3>Dr. P. M.</h3>
        <p>Our game designer, who crafts the challenges that players face in Wikispeedia.</p>
    </div>
    <div class="team-card" style={{ width: '300px' }}>
        <img src="/pp/lp.webp" alt="Dr. L. P." />
        <h3>Dr. L. P.</h3>
        <p>Our anthropologist, who studies the social dynamics of Wikispeedia players.</p>
    </div>
</div>

Together, we form **ADAspeedia**, a team united in exploring how machines can emulate and evolve human behavior. To see the real faces of our team, click [here](/team). You can also view our real functions in the [readme](https://github.com/epfl-ada/ada-2024-project-adaspeedia/blob/main/README.md).

# A Human Blueprint 
To recreate human-like behavior, we first need to understand how humans played Wikispeedia. What strategies did they use? Did they wander aimlessly or systematically? How did they balance the exploration of diverse topics with the focused goal of reaching their destination? These questions guided our initial analysis of the human dataset, revealing the intricacies of the paths taken, the cognitive shortcuts employed, and the common hurdles encountered.

For this, we have data extracted from the Previous Age. A team of researchers from EPFL has collected thousands of paths taken by humans, [unified in a single dataset](https://snap.stanford.edu/data/wikispeedia.html).  
We've analyzed it to keep only the essentials

### Articles
This is the data containing the name and data contained in each article. In total we have access to 4604 different articles.

### Links
This is the list of outgoing links from a given source article.
![](/charts/wikispeedia_articles_links.png)
On average, articles have around 26 outgoing links, the median being at 19. Half the articles have between 11 and 33 outgoing links. The United States article has the most outgoing links (294), and each article has at least 1 outgoing link. The distribution of the number of links per article is right-skewed.
We can also look at this data the other way around: based on the articles at our disposal, how many incoming links are there per articles? This can give us an idea of how reachable articles are.
![](/charts/wikispeedia_articles_incoming_links.png)
We can note that articles are always reachable within our given set. The median number of incoming links is of 10, while the mean is of 29 incoming links. Some severe outliers, such as the United States article again with 1551 incoming links, skew the distribution to the right.

### Categories
These refer to the classification of each article based on high level concepts they are each related to. In total, 129 categories are available in our dataset. 88% of articles are linked to one single category, around 11% have two and less than 1% have 3. This reveal different level of granularity and lack of coherence in the dataset. For example, 'United_States' is linked to Countries and North American Geography. 'Great_Britain', on the other hand, is linked to 'British History' and 'Politics and government', but no 'European Geography'.
On average categories have 40 articles. If we now look at the distribution of the categories, we see it is not spread evenly.
![](/charts/wikispeedia_categories_distrib.png)



### Paths
The paths are sequences of articles visited by players (id & title of the articles separated by a comma). They can be subdivided in the success paths (Paths finished) and the failure ones (Paths unfinished).
In total we have access to 28718 finished paths and 24875 unfinished ones.
#### Paths finished
On average humans take 6 steps to get to the goal. A few outliers are present, such as the path with the maximum number of steps (434) to go from 'United_States' to 'English_language', or the second to first, taking 118 steps to go from Napoleon_I_of_France to Recycling.
![](/charts/humans_paths_finished_steps.png)

Do people take a long time to finish a path? The average time is 2min 41s. The longest time is 9h 58min 21s. The shortest time is 0 seconds, which for paths finished can only mean that the goal and start article were the same. Most of the players (80th percentile) took less than 3 minutes, 31 seconds to finish a game.

Another info we have are the ratings: These are user submitted grades between 1 and 5 representing the level of difficulty felt on the given path. This information is optional, hence we have less values to base our description on (approx. 55%).

Our two indicators of difficulty are the path length and the ratings. The latter being a user-centered metric, based on subjective feelings, while the former is more objective, focusing on performance. An interesting thing to look at is to check if a correlation might exist between the two.

![](/charts/wikispeedia_pathlength_rating.png)

Obviously a positive correlation appears visually, but this is also due to outliers in the data. If we remove them, by taking only the path lengths under the 80th percentile (13), we get the following plot:
![](/charts/wikispeedia_pathlength_rating_q80.png)
When computing the Spearman R correlation coefficient, we find a positive correlation of 0.46 with a p-value at 0.0. This clearly shows that in our dataset, path length and ratings are positively corellated.

#### Paths unfinished
Within the 24875 unfinished paths, there is actually only 18190 unique ones, the other being duplicate attempts at the same source-goal pair of articles.
We found 4200 pairs of source-goal articles in common with the finished paths. We wanted to dig deeper and look at potential difficulty markers.
![](/charts/wikispeedia_unfinished_common_rating.png)
Here we don't see any positive correlation. Hence we can't definitely relate ratings given by users to the difficulty of unfinished paths.

Players can forfeit their attempt in two different ways: either by timing out after 30 minutes of inactivity, or by restarting manually. There is overall 64.5% of restarts and 35.5% of timeouts.

# Building our machine

To ensure our AI models are as reliable as possible while minimizing computation time, we have chosen the GPT-4o mini model from 2024, which is still operational today in one of the decommissioned datacenters we have repurposed. At the time, it was the most efficient and cost-effective model, making it ideal for this type of task.  

We will later compare it to another model, MistralAI Large-2407, also from 2024, which we have run using our emulators.  

Our analysis will be conducted in two distinct ways: with and without memory for the LLM. Initially, we will run an LLM without memory on a unique subset of human paths. There are 28,718 such paths, meaning our first LLM without memory will generate 28,718 paths based on the following prompt:  

[prompt box]

To avoid loops (which can easily occur when the LLM lacks memory), we deliberately choose to halt exploration whenever a loop is detected. The complete code is available here.  

# The digital mind's capabilities: A comparative analysis

A good first indicator of our machine's capabilities is to look at the lengths of the paths completed.
As a reminder, we've already seen previously that human paths are on average 6 steps long, with a maximum of 434.
![](/charts/gpt4omni_no_memory_paths_finished_steps.png)
The median for the paths created by the LLM without memory is of 3 steps with a maximum of 16. Clearly, LLMs tend to complete paths faster than humans, showing that the machine is clearly playing better than humans. However, as previously stated outliers are present within the human dataset. This may hinder our ability to truly visualize how both distributions are. To correct that, we suggest looking at the value up to the 99th percentile.

![](/charts/gpt4omni_no_memory_humans_98th_percentile_paths_finished_steps.png)

Over here, we can see more clearly how much both distributions are different. The human paths are spreading more towards the right, i.e. they are more lenghier in general.
We can look at specific examples. United_States -> Abraham_Lincoln. A human mind might first think about Abraham Lincoln being President of the United States, and hence take the path United_States -> President_of_the_United_States -> Abraham_Lincoln. Our machine abstracts these unnecessary connections and "sees" that Abraham_Lincoln is directly reacheable from United_States.
As a first indicator, this shows that our machine is able to think more efficiently than humans. It is able to link two notions together in smaller steps, displaying great skills at this game.


   <Alert variant="info">
        <AlertTitle>Note</AlertTitle>
        <AlertDescription>You can use our to compare how both LLM and humans navigated different paths</AlertDescription><br />
        <Button>ðŸ”Ž Explore Paths</Button>
    </Alert>

Now, we're interested to see if the machine can succeed where humans fail. We therefore took the paths where humans previously failed, i.e. unique pairs of start and goal articles that have never been finished by humans. We ran our machine on these 1396 paths. Overall we get the following distribution of outcomes.

![](/charts/piechart_llm_on_unfinished.png)

"Wrong answer" refers to the machine trying to access a link that is not part of the ones available at a given step, "Dead end" refers to arriving at an article with no outgoing links and "Loop detected" means that the machine is stuck in a loop.
Here, the machine has been able to finish 30% of paths that humans were previously unable to complete. This shows our machine's superiority against the human race. To go even further, we decided to look at the distribution of the lengths on these new completed paths.

![](/charts/llm_finished_vs_unfinished.png)

We clearly see that overall, our machine takes more steps on these paths than on the previous ones, with a median of 5 steps. This shows that these paths requires more raffined thinking, which can explain why humans failed.

As previously mentionned, humans usually adopt a strategy of getting away then homing in, passing through hubs. Let's see if the most visited articles are similar.

![](/charts/most_visited_articles_human.png)
![](/charts/most_visited_articles_llm.png)

Humans seem to focus on various geographical locations more. In contrast, the machine appears more focused on scientific concepts. Still, some articles appear on both, such as United_States. When looking at the connectivity of the Wikipedia article for United States, it reveals 1551 incoming and 294 outgoing links. This mean that the U.S. article is a great hub that can be easily accessible and lead to many useful links. 
The fact that it is less used than the Wikipedia article for "Animal" on the machine side seems surprising. However, this might reveal some implicit bias from the LLM towards the United_States article. It may not want to use that article as it might have been trained to avoid controversial notions and stay on more "neutral" concepts such as science and nature. This can explain why it prefers going through 'Animal' despite that article being 3 times less reacheable and leading to 10 times less articles than 'United_States'. Nonetheless, the 'United_States' article is still a close second, which shows how useful and unavoidable this article is.


As you can see, our initial basic protocol leads to a significant number of paths left unexplored to completion, largely due to detected loops.  
As a further step, we run our LLM with memory on this specific subset of unfinished paths, along with a sample of standard paths, to compare the performance of the memoryless LLM against the one with memory. In total, 13,411 paths are processed by our system.

# Side research: other models?
Do you want to see more ? From the beginning we saw how GPT-4 plays Wikispeedia. How about other LLMs?

Did you here about MistralAI. It is an European leader in artificial intelligence. We chose this llm and made it follow the same protocol as our first one, without any memory. Let's see how it does.

Using the Z-score, for a 95% confidence interval, the minimum number of paths needed to get a statistically significant result is 385. We added a bit more to add to the significance and therefore take randomly 565 rows from the dataset. 

![](/charts/nbr_steps_openai_mistral.png)
For the finished paths, we compared the number of steps taken and we can see that Mistral takes generally less steps to get to the target article, showing a better efficiency. Let's look deeper into the comparison of success and failures for both models.

![](/charts/piechart_openai.png)
![](/charts/piechart_mistralai.png)

We can see that for both models the success rate is similar. However, OpenAI's GPT-4 tends to loop more and MistralAI tends to give more wrong answers. This can show differents digital minds, i.e. different abilities for a given task. From that we can speculate that MistralAI might have a bigger temperature, leading to more randomness in the answers and hence less predictable or correct results.


# The machine's understanding of the human language

Prior to our study, other methods already existed to compute the semantic distance between two words. Indeed, language models usually use a system of *word embeddings*, which is a specific representation of words and notions. An interesting point with these embeddings is that they can be visualized as high-dimensionnal vectors. They can hence be compared together with simpler comparison tools, such as the euclidean L2 distance.
Previous studies show that the embeddings learned by models usually capture the semantic behind words. For example, as embeddings are vectors, we can add them up together. It turns out that the result of the operation 'Greece' - 'Athens' + 'Paris' yields a very similar vector to the one representing 'France'.
We are now interested in the similarities in distribution between the difference in distances computed on paths played by humans VS our machine, and the difference in distances computed on paths played by humans VS existing human embeddings.

We first used a Bidirectional Encoder Representations from Transformers (BERT) base model. This architecture has proven to be consistantly powerful on multiple natural language processing tasks, specifically those related to semantic understanding. Hence, it seems like the perfect fit for our task.

To study the distribution of differences, we followed the following protocol:

- Compute the distances for the paths played by humans and by our machine.

- Compute the difference of distance for each path in common.

- Extact the pair of articles.

- Compute for each of the pairs the BERT embeddings of each article.

- Based on the embeddings, compute the euclidean distance between two article names.

- Compute the difference with the distances for the paths played by humans


![Distribution of difference between human computed distances and LLM computed distances](/charts/difference_humans_llm.png)
![Distribution of difference between human computed distances and embeddings euclidean distance](/charts/difference_humans_euclidean.png)

For the distribution of difference between human computed and machine computed distances, we can see that the distribution of differences between human and LLM computed distances resembles a Laplace distribution, which clearly indicates a strong alignment between human and machine understanding of semantic relationships. Indeed, this is visible because the mean, median and interquartile range are all very close to 0.
However, the distribution is still spreading a bit, suggesting that the machine may deviate a bit more from human understanding. Besides the obvious outliers previously described (English_language), the largest difference in absolute value suggest different priorities when computing the semantic difference. For example, the difference in the human distance for Canada and Japan and the machine distance shows that the machine potentially thinks more abstractly, i.e. these are two countries, hence the concepts are very similar. However, humans may give more importance to other specificities, such as the geographic distance, lack of common history themes, etc...

Now, looking at the distribution of the difference between human computed distances and embeddings, the data looks more Gaussian, with a mean and median around -6. This means that, distances computed by humans paths are usually way lower than the ones computed by humans, showing discrepancy in the understanding of the human language.

Therefore, we can overall conclude that our machine aligns more with humans in the distance computed based on the paths it plays, hence it mimics the human behavior as expected.
