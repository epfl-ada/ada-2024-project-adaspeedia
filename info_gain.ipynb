{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "import pandas as pd\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data from the tsv files, skipping the headers and specifying column names\n",
    "DATA_FOLDER = 'data/wikispeedia_paths-and-graph/'\n",
    "articles = pd.read_csv(DATA_FOLDER + 'articles.tsv', sep='\\t', skiprows=12, names=['article'])\n",
    "categories = pd.read_csv(DATA_FOLDER + 'categories.tsv', sep='\\t', skiprows=12, names=['article', 'category'])\n",
    "links = pd.read_csv(DATA_FOLDER + 'links.tsv', sep='\\t', skiprows=11, names=['linkSource', 'linkTarget'])\n",
    "paths_finished = pd.read_csv(DATA_FOLDER + 'paths_finished.tsv', sep='\\t', skiprows=15, names=['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating'])\n",
    "paths_unfinished = pd.read_csv(DATA_FOLDER + 'paths_unfinished.tsv', sep='\\t', skiprows=16, names=['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'target', 'type'])\n",
    "paths_openai = pd.read_csv('data/merged_file_final_openai.tsv', sep='\\t', names=['path_id', 'steps', 'path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the URL-encoded article titles\n",
    "articles = articles.map(urllib.parse.unquote)\n",
    "categories = categories.map(urllib.parse.unquote)\n",
    "links = links.map(urllib.parse.unquote)\n",
    "paths_finished['path'] = paths_finished['path'].map(urllib.parse.unquote)\n",
    "paths_unfinished['path'] = paths_unfinished['path'].map(urllib.parse.unquote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior click probability\n",
    "\n",
    "# The probability to click on each of article a’s L_a outlinks is the same for all outlinks\n",
    "# So we don’t need a third dimension to index the specific outlinks\n",
    "\n",
    "# Actually, it doesn’t depend either on the goal, because our prior is that the links are clicked randomly\n",
    "\n",
    "# Count the outlinks of each article to get the probability of clicking on any of them\n",
    "out_degree = links.groupby('linkSource').size()\n",
    "probs_prior = 1 / out_degree\n",
    "# Create a Series indexed by 'linkSource' that gives a list of all the source’s outlinks\n",
    "out_links = links.groupby('linkSource')['linkTarget'].agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N(A=a, G=g): the number of times 'a' was encountered on paths for which 'g' was the goal\n",
    "count_goal_article = defaultdict(lambda: defaultdict(int))\n",
    "# N(A’=a’, A=a, G=g): the number of times a’ was clicked in this situation\n",
    "count_goal_article_article_clicked = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "# Filter out the paths that have only one article, because I have no clue what they mean\n",
    "paths = paths_finished['path'][paths_finished['path'].apply(len) > 1]\n",
    "\n",
    "# Create simplified paths by getting rid of the backtracking steps, and going straight\n",
    "# where the player ended up going after backtracking\n",
    "def straighten_path(path):\n",
    "    stack = deque()\n",
    "    for article in path:\n",
    "        if article == '<':\n",
    "            stack.pop()\n",
    "        else:\n",
    "            stack.append(article)\n",
    "    return list(stack)\n",
    "\n",
    "paths_no_backtrack = paths.apply(straighten_path)\n",
    "\n",
    "# Count the occurrences of a, a’ and g along every path\n",
    "for path in paths_no_backtrack:\n",
    "    goal = path[-1]\n",
    "    # Iterate through the path by getting each time one article and the one that was clicked from it.\n",
    "    # It starts at (start_article, first_article_clicked) and ends with (before_last_article, goal).\n",
    "    for article, article_clicked in zip(path, path[1:]):\n",
    "        count_goal_article[goal][article] += 1\n",
    "        count_goal_article_article_clicked[goal][article][article_clicked] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N(A=a, G=g): the number of times 'a' was encountered on paths for which 'g' was the goal\n",
    "count_goal_article = defaultdict(lambda: defaultdict(int))\n",
    "# N(A’=a’, A=a, G=g): the number of times a’ was clicked in this situation\n",
    "count_goal_article_article_clicked = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "# Filter out the paths that have only one article, because I have no clue what they mean\n",
    "paths_openai = paths_openai['path'][paths_openai['path'].apply(len) > 1]\n",
    "\n",
    "\n",
    "# Count the occurrences of a, a’ and g along every path\n",
    "for path in paths_openai:\n",
    "    goal = path[-1]\n",
    "    # Iterate through the path by getting each time one article and the one that was clicked from it.\n",
    "    # It starts at (start_article, first_article_clicked) and ends with (before_last_article, goal).\n",
    "    for article, article_clicked in zip(path, path[1:]):\n",
    "        count_goal_article[goal][article] += 1\n",
    "        count_goal_article_article_clicked[goal][article][article_clicked] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Python3.13\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g, article_counts \u001b[38;5;129;01min\u001b[39;00m count_goal_article\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a, count \u001b[38;5;129;01min\u001b[39;00m article_counts\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m a_ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mout_links\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m      9\u001b[0m             \u001b[38;5;66;03m# Use the formula (1) from the Wikispeedia paper\u001b[39;00m\n\u001b[0;32m     10\u001b[0m             probs_posterior[g][a][a_] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     11\u001b[0m                     (count_goal_article_article_clicked[g][a][a_] \u001b[38;5;241m+\u001b[39m alpha_)\n\u001b[0;32m     12\u001b[0m                     \u001b[38;5;241m/\u001b[39m\n\u001b[0;32m     13\u001b[0m                     (count_goal_article[g][a] \u001b[38;5;241m+\u001b[39m alpha_ \u001b[38;5;241m*\u001b[39m out_degree[a])\n\u001b[0;32m     14\u001b[0m             )\n",
      "File \u001b[1;32md:\\Python3.13\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32md:\\Python3.13\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32md:\\Python3.13\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '1'"
     ]
    }
   ],
   "source": [
    "# Posterior click probabilities\n",
    "probs_posterior = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "# alpha_ is the Dirichlet parameter representing the initial confidence in the uniform prior distribution\n",
    "alpha_ = 0.1\n",
    "\n",
    "for g, article_counts in count_goal_article.items():\n",
    "    for a, count in article_counts.items():\n",
    "        for a_ in out_links[a]:\n",
    "            # Use the formula (1) from the Wikispeedia paper\n",
    "            probs_posterior[g][a][a_] = (\n",
    "                    (count_goal_article_article_clicked[g][a][a_] + alpha_)\n",
    "                    /\n",
    "                    (count_goal_article[g][a] + alpha_ * out_degree[a])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_prior_entropy(path):\n",
    "    \"\"\"\n",
    "    Turn a path into the prior entropy at each article along the path\n",
    "    \n",
    "    Parameters:\n",
    "        path (array of strings): list of the titles of the articles along the path\n",
    "        \n",
    "    Returns:\n",
    "        entropies (array of floats): list of the prior entropy values for each article along this path\n",
    "    \"\"\"\n",
    "    # Skip the goal because the entropy is 0 at the goal\n",
    "    return [-1 * out_degree[a] * probs_prior[a] * np.log(probs_prior[a]) for a in path[:-1]]\n",
    "\n",
    "def path_to_posterior_entropy(path):\n",
    "    \"\"\"\n",
    "    Turn a path into the posterior entropy at each article along the path\n",
    "    \n",
    "    Parameters:\n",
    "        path (array of strings): list of the titles of the articles along the path\n",
    "        \n",
    "    Returns:\n",
    "        entropies (array of floats): list of the posterior entropy values for each article along this path\n",
    "    \"\"\"\n",
    "    g = path[-1]\n",
    "    # Skip the goal because the entropy is 0 at the goal\n",
    "    return [(-1 * sum([prob * np.log(prob) for prob in probs_posterior[g][a].values()])) for a in path[:-1]]\n",
    "\n",
    "entropies_prior = paths_no_backtrack.apply(path_to_prior_entropy)\n",
    "entropies_posterior = paths_no_backtrack.apply(path_to_posterior_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entropies_prior' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m\n\u001b[0;32m     48\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxticks(rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m plt\u001b[38;5;241m.\u001b[39mgcf()\n\u001b[1;32m---> 52\u001b[0m plot_normalized_positions(\u001b[43mentropies_prior\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprior entropy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     53\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     54\u001b[0m plot_normalized_positions(entropies_posterior, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposterior entropy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'entropies_prior' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize the entropies like in Fig. 2 of the Wikispeedia paper\n",
    "def plot_normalized_positions(series, graph_title, n_bins=7):\n",
    "    \"\"\"\n",
    "    Create a bar plot of binned averages along the length of an array,\n",
    "    but plotted along an x-axis normalized to [0,1].\n",
    "    \n",
    "    Parameters:\n",
    "    series: pandas.Series where each element is an array of numbers\n",
    "    graph_titre: string with the name of the quantity plotted\n",
    "    n_bins: number of bins to divide the [0,1] interval into\n",
    "    \"\"\"\n",
    "    # Create empty lists to store normalized positions and values\n",
    "    all_positions = []\n",
    "    all_values = []\n",
    "\n",
    "    # Process each array in the series\n",
    "    for arr in series:\n",
    "        length = len(arr)\n",
    "        # Create normalized positions for this array\n",
    "        positions = np.linspace(0, 1, length)\n",
    "\n",
    "        all_positions.extend(positions)\n",
    "        all_values.extend(arr)\n",
    "\n",
    "    # Create a DataFrame with the normalized positions and values\n",
    "    df = pd.DataFrame({\n",
    "        'position': all_positions,\n",
    "        'value': all_values\n",
    "    })\n",
    "\n",
    "    # Create bins and calculate statistics for each bin\n",
    "    df['bin'] = pd.cut(df['position'], bins=n_bins, labels=[f'{i/n_bins:.2f}-{(i+1)/n_bins:.2f}' for i in range(n_bins)])\n",
    "\n",
    "    bin_stats = df.groupby('bin', observed=True).agg({\n",
    "        'value': ['mean']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten the column names\n",
    "    bin_stats.columns = ['bin', 'mean']\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    sns.barplot(data=bin_stats, x='bin', y='mean')\n",
    "\n",
    "    plt.title(f'Average {graph_title} along normalized path distance')\n",
    "    plt.xlabel('Normalized distance along the path')\n",
    "    plt.ylabel('Average bits of information')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    return plt.gcf()\n",
    "\n",
    "plot_normalized_positions(entropies_prior, 'prior entropy')\n",
    "plt.show()\n",
    "plot_normalized_positions(entropies_posterior, 'posterior entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior click probabilities\n",
    "probs_posterior = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "# alpha_ is the Dirichlet parameter representing the initial confidence in the uniform prior distribution\n",
    "alpha_ = 0.1\n",
    "\n",
    "for g, article_counts in count_goal_article.items():\n",
    "    for a, count in article_counts.items():\n",
    "        for a_ in out_links[a]:\n",
    "            # Use the formula (1) from the Wikispeedia paper\n",
    "            probs_posterior[g][a][a_] = (\n",
    "                    (count_goal_article_article_clicked[g][a][a_] + alpha_)\n",
    "                    /\n",
    "                    (count_goal_article[g][a] + alpha_ * out_degree[a])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to subtract two lists element-wise\n",
    "def subtract_lists(list1, list2):\n",
    "    return [a - b for a, b in zip(list1, list2)]\n",
    "\n",
    "# Subtract posterior entropy to prior entropy element-wise in each path to obtain information gain\n",
    "information_gain = entropies_prior.combine(entropies_posterior, subtract_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normalized_positions(information_gain, 'information gain')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
