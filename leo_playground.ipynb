{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279a5c4e3b397aa9",
   "metadata": {},
   "source": [
    "## 1. Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2a927188a1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from src.utils.plot import plot_normalized_positions\n",
    "from src.utils.paths import paths_no_backtrack\n",
    "from src.utils.probs import posterior_probabilites, entropies_prior_posterior, information_gain\n",
    "from src.utils.distances import compute_distances\n",
    "from src.utils.load_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles, categories, links, paths_finished, paths_finished_llm, paths_unfinished, paths_unfinished_llm_df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d89a1a85e10e711",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "We found missing values for the rating of finished paths and decided not to impute them as replacing missing values with the median rating would not make sense. We will only use finished paths with a human rating and discard all other finished paths when we need to make analysis based on the rating.\n",
    "\n",
    "### Handling the data size\n",
    "\n",
    "Our pipeline can handle the datasize as we do not need to handle bigger datasets than the ones we have already treated through this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1729f5e633a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing descriptive statistics for key variables\n",
    "\n",
    "paths_finished_len = paths_finished['path'].apply(len)\n",
    "print(paths_finished_len.describe())\n",
    "print(paths_finished['rating'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e602a52806671b6",
   "metadata": {},
   "source": [
    "### Exploring correlations\n",
    "\n",
    "Here we evaluate the correlation between the difficulty and the path length for human games. \n",
    "We observe that the rating is positively correlated with the path length.\n",
    "R2 shows that our model explains some of the variance of the data.\n",
    "p is close to 0 which shows that the rating is a good predictor for the path length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a474c6ff163a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_finished['path_length'] = paths_finished_len\n",
    "mod = smf.ols(formula='path_length ~ rating', data=paths_finished)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fcf56f6b23c19b",
   "metadata": {},
   "source": [
    "## 2. Compute probabilities and entropies\n",
    "\n",
    "### 2.A. Prior click probability\n",
    "\n",
    "![Formula for the prior click probability](./images/probs_prior.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec893e2ab380ea",
   "metadata": {},
   "source": [
    "The prior click probability only depends on the number of outlinks of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f37f809c6bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the out-links of each article to get the probability of clicking on any of them\n",
    "out_degree = links.groupby('linkSource').size()\n",
    "probs_prior = 1 / out_degree\n",
    "# Create a Series indexed by 'linkSource' that gives a list of all the source’s out-links\n",
    "out_links = links.groupby('linkSource')['linkTarget'].agg(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3613fd1a1d169e6",
   "metadata": {},
   "source": [
    "### 2.B. Posterior click probability\n",
    "\n",
    "![Formula for the posterior click probability](./images/probs_posterior.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e47d883519e1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_finished = paths_no_backtrack(paths_finished['path'])\n",
    "probs_posterior = posterior_probabilites(paths_finished, out_links, out_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d986b1468c4a90",
   "metadata": {},
   "source": [
    "### 2.C. Prior and posterior entropies\n",
    "\n",
    "Compute the prior and posterior entropy at each article along the path, except for the goal, because entropy is 0 once we have reached the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad4dab0c8590d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_prior, entropies_posterior = entropies_prior_posterior(paths_finished, probs_prior, probs_posterior, out_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211d7624e89eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the entropies like in Fig. 2 of the Wikispeedia paper\n",
    "plot_normalized_positions(entropies_prior, 'prior entropy')\n",
    "plot_normalized_positions(entropies_posterior, 'posterior entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e61d6d1b59351",
   "metadata": {},
   "source": [
    "## 3. Split the paths\n",
    "\n",
    "### 3.A. Compute “information gain”\n",
    "\n",
    "This represents how much information we gain by looking at the click distribution, as described in section 4 and figure 2 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180a59e146aa531",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain = information_gain(entropies_prior, entropies_posterior)\n",
    "plot_normalized_positions(info_gain, 'information gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea43686e9bd4cd",
   "metadata": {},
   "source": [
    "This is similar to what the paper gets:\n",
    "\n",
    "![information gain graph from the paper](./images/fig_2_paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942b33c50b8771e",
   "metadata": {},
   "source": [
    "### 3.B. Get the splitting point and split the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4bd099cce4dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all the “homing in” parts of the paths in the array, by splitting\n",
    "# the paths at the point of minimum information\n",
    "\n",
    "# Get the index of the point of lowest information gain for each path\n",
    "argmin_info_gain = info_gain.apply(np.argmin)\n",
    "# Split the path at this point and keep only the second part, corresponding to the “homing in” phase\n",
    "paths_homing_in = pd.Series([path[argmin_info_gain[i]:] for i, path in paths_finished.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef5f3be3bd04c6",
   "metadata": {},
   "source": [
    "## 4. Compute embedding distances\n",
    "\n",
    "We now need to get a list of all the article titles whose embedding we need to compute, as well as the list of pairs of articles between which we need to compute the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c27426a11a450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the set of all articles encountered in the paths_homing_in\n",
    "# Every article encountered along a goal has a defined distance with the goal\n",
    "all_distance_pairs = set()\n",
    "all_articles = set()\n",
    "for path in paths_homing_in:\n",
    "    goal  = path[-1]\n",
    "    for article in path:\n",
    "        all_articles.add(article)\n",
    "        all_distance_pairs.add((article, goal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6ea0652cbcba8",
   "metadata": {},
   "source": [
    "We use the sentence-transformers library to load the pre-trained BERT model and compute embeddings for each of our article titles. Then, we compute distances between these embeddings models, using both cosine similarity and Euclidean distance, which we return in a dataframe `similarities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abef8c191933114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.generate_embeddings_distances import get_embeddings_distances\n",
    "similarities = get_embeddings_distances(all_articles, all_distance_pairs, 'data/article_similarities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f0818873c29c2",
   "metadata": {},
   "source": [
    "## 5. Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c97eb2060d4fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_humans = compute_distances(links, probs_posterior, paths_homing_in, 'data/distances_humans.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2b045",
   "metadata": {},
   "source": [
    "## 6. Get wikispeedia distances from LLM games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63923d6",
   "metadata": {},
   "source": [
    "First, we need to verify that the Wikispeedia distance can be computed in the same way for LLMs. We check if the LLMs adopt the same strategy of getting out-homing which is at the basis of our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450deca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Question 2'''\n",
    "\n",
    "# Compute posterior probabilities\n",
    "probs_posterior_llm = posterior_probabilites(paths_finished_llm, out_links, out_degree)\n",
    "\n",
    "# Compute and plot entropies\n",
    "entropies_prior_llm, entropies_posterior_llm = entropies_prior_posterior(paths_finished_llm, probs_prior, probs_posterior_llm, out_degree)\n",
    "plot_normalized_positions(entropies_prior, 'prior entropy')\n",
    "plot_normalized_positions(entropies_posterior, 'posterior entropy')\n",
    "\n",
    "# Get llm information gain\n",
    "info_gain_llm = information_gain(entropies_prior_llm, entropies_posterior_llm)\n",
    "plot_normalized_positions(info_gain_llm, 'information gain')\n",
    "\n",
    "# Split the paths\n",
    "\n",
    "# Get the index of the point of lowest information gain for each path\n",
    "argmin_info_gain_llm = info_gain_llm.apply(np.argmin)\n",
    "# Split the path at this point and keep only the second part, corresponding to the “homing in” phase\n",
    "paths_homing_in_llm = pd.Series([path[argmin_info_gain_llm[i]:] for i, path in paths_finished_llm.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9344965",
   "metadata": {},
   "source": [
    "In the following code, we extract the Wikispeedia semantic distances from the corresponding finished paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_llm = compute_distances(links, probs_posterior_llm, paths_finished_llm, 'data/distances_llm.pkl')\n",
    "\n",
    "distance_differences = dict()\n",
    "common_distance_pairs = set(distances_llm.keys()) & set(distances_humans.keys())\n",
    "for pair in common_distance_pairs:\n",
    "    distance_differences[pair] = distances_humans[pair] - distances_llm[pair]\n",
    "\n",
    "import seaborn as sns\n",
    "sns.histplot(distance_differences.values())\n",
    "\n",
    "common_distances_humans = pd.Series([distances_humans[pair] for pair in common_distance_pairs])\n",
    "common_distances_llm = pd.Series([distances_llm[pair] for pair in common_distance_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c4c2c18cc0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(common_distances_humans.describe())\n",
    "print(common_distances_llm.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_distances_humans = pd.Series(distances_humans)\n",
    "series_distances_llm = pd.Series(distances_llm)\n",
    "\n",
    "# distances to csv\n",
    "distances_humans_df = pd.DataFrame(list(distances_humans.items()), columns=['pair', 'distance'])\n",
    "distances_humans_df.to_csv('data/distances_humans.csv', index=False)\n",
    "\n",
    "distances_llm_df = pd.DataFrame(list(distances_llm.items()), columns=['pair', 'distance'])\n",
    "distances_llm_df.to_csv('data/distances_llm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
