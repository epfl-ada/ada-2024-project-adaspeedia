{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model_name = \"bert-base-uncased\"  # TODO: Try different models\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad(): # we are not training the model, so we don't need gradients\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.7934294939041138\n"
     ]
    }
   ],
   "source": [
    "def calculate_cosine_similarity(title1, title2):\n",
    "    embedding1 = get_embedding(title1)\n",
    "    embedding2 = get_embedding(title2)\n",
    "    embedding1 = embedding1.reshape(1, -1)\n",
    "    embedding2 = embedding2.reshape(1, -1)\n",
    "\n",
    "    similarity = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "    return similarity\n",
    "\n",
    "# Example usage\n",
    "title1 = \"Understanding Machine Learning\"\n",
    "title2 = \"Introduction to Deep Learning\"\n",
    "similarity = calculate_cosine_similarity(title1, title2)\n",
    "print(f\"Cosine Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6303218603134155\n"
     ]
    }
   ],
   "source": [
    "article1= \"14th_century\"\n",
    "article2= \"African_slave_trade\"\n",
    "similarity = calculate_cosine_similarity(article1, article2)\n",
    "print(f\"Cosine Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9655939936637878\n"
     ]
    }
   ],
   "source": [
    "article1= \"14th_century\"\n",
    "article2= \"15th_century\"\n",
    "similarity = calculate_cosine_similarity(article1, article2)\n",
    "print(f\"Cosine Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.7063847780227661\n"
     ]
    }
   ],
   "source": [
    "article1_2= \"14th_century\"\n",
    "article2_2= \"Ottoman_Empire\"\n",
    "similarity = calculate_cosine_similarity(article1_2, article2_2)\n",
    "print(f\"Cosine Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from sentence_transformers) (4.42.0.dev0)\n",
      "Requirement already satisfied: tqdm in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from sentence_transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from sentence_transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from sentence_transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from sentence_transformers) (0.23.3)\n",
      "Requirement already satisfied: Pillow in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from sentence_transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.10.0)\n",
      "Requirement already satisfied: sympy in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.25.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "   ---------------------------------------- 0.0/255.8 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/255.8 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/255.8 kB 330.3 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 61.4/255.8 kB 409.6 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 122.9/255.8 kB 654.9 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 204.8/255.8 kB 888.4 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 225.3/255.8 kB 860.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 255.8/255.8 kB 826.7 kB/s eta 0:00:00\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_sbert_similarity(title1, title2):\n",
    "    # Get embeddings\n",
    "    embedding1 = model.encode(title1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(title2, convert_to_tensor=True)\n",
    "    # Calculate cosine similarity using SBERT's util function\n",
    "    similarity = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity with SBERT: 0.5268170833587646\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "title1_1 = \"Understanding Machine Learning\"\n",
    "title2_1 = \"Introduction to Deep Learning\"\n",
    "similarity = calculate_sbert_similarity(title1, title2)\n",
    "print(f\"Cosine Similarity with SBERT: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity with SBERT: 0.9229943156242371\n"
     ]
    }
   ],
   "source": [
    "article1_1= \"14th_century\"\n",
    "article2_1= \"15th_century\"\n",
    "similarity = calculate_sbert_similarity(article1_1, article2_1)\n",
    "print(f\"Cosine Similarity with SBERT: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity with SBERT: 0.30906590819358826\n"
     ]
    }
   ],
   "source": [
    "article1_3= \"14th_century\"\n",
    "article2_3= \"African_slave_trade\"\n",
    "similarity = calculate_sbert_similarity(article1_3, article2_3)\n",
    "print(f\"Cosine Similarity with SBERT: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistralai in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from mistralai) (0.2.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from mistralai) (0.27.2)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from mistralai) (1.0.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from mistralai) (2.9.2)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from mistralai) (2.8.2)\n",
      "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from mistralai) (0.9.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from python-dateutil==2.8.2->mistralai) (1.16.0)\n",
      "Requirement already satisfied: anyio in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.0.6)\n",
      "Requirement already satisfied: idna in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (3.6)\n",
      "Requirement already satisfied: sniffio in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->mistralai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (4.10.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from typing-inspect<0.10.0,>=0.9.0->mistralai) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\ghita\\anaconda\\envs\\modern_nlp\\lib\\site-packages (from anyio->httpx<0.28.0,>=0.27.0->mistralai) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "%pip install mistralai\n",
    "\n",
    "\n",
    "from mistralai import Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Det\n",
      "erm\n",
      "ining\n",
      " the\n",
      " \"\n",
      "best\n",
      "\"\n",
      " French\n",
      " cheese\n",
      " can\n",
      " be\n",
      " subject\n",
      "ive\n",
      ",\n",
      " as\n",
      " it\n",
      " largely\n",
      " depends\n",
      " on\n",
      " personal\n",
      " taste\n",
      ".\n",
      " France\n",
      " is\n",
      " renown\n",
      "ed\n",
      " for\n",
      " its\n",
      " wide\n",
      " variety\n",
      " of\n",
      " che\n",
      "es\n",
      "es\n",
      ",\n",
      " with\n",
      " estimates\n",
      " suggesting\n",
      " there\n",
      " are\n",
      " over\n",
      " \n",
      "1\n",
      ",\n",
      "0\n",
      "0\n",
      "0\n",
      " different\n",
      " types\n",
      ".\n",
      " Here\n",
      " are\n",
      " a\n",
      " few\n",
      " highly\n",
      " regarded\n",
      " French\n",
      " che\n",
      "es\n",
      "es\n",
      " across\n",
      " various\n",
      " categories\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " **\n",
      "Soft\n",
      " Che\n",
      "es\n",
      "es\n",
      ":**\n",
      "\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "B\n",
      "rie\n",
      " de\n",
      " Me\n",
      "aux\n",
      "**:\n",
      " K\n",
      "nown\n",
      " for\n",
      " its\n",
      " cream\n",
      "y\n",
      " texture\n",
      " and\n",
      " rich\n",
      ",\n",
      " earth\n",
      "y\n",
      " flavor\n",
      ".\n",
      "\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Cam\n",
      "ember\n",
      "t\n",
      " de\n",
      " Norm\n",
      "and\n",
      "ie\n",
      "**:\n",
      " A\n",
      " soft\n",
      ",\n",
      " cream\n",
      "y\n",
      " cheese\n",
      " with\n",
      " a\n",
      " strong\n",
      ",\n",
      " distinctive\n",
      " taste\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      ".\n",
      " **\n",
      "S\n",
      "emi\n",
      "-\n",
      "Soft\n",
      " Che\n",
      "es\n",
      "es\n",
      ":**\n",
      "\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "M\n",
      "orb\n",
      "ier\n",
      "**:\n",
      " Rec\n",
      "ogn\n",
      "izable\n",
      " by\n",
      " its\n",
      " layer\n",
      " of\n",
      " ash\n",
      " in\n",
      " the\n",
      " middle\n",
      ",\n",
      " it\n",
      " has\n",
      " a\n",
      " nut\n",
      "ty\n",
      " and\n",
      " fru\n",
      "ity\n",
      " flavor\n",
      ".\n",
      "\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Reb\n",
      "lo\n",
      "ch\n",
      "on\n",
      "**:\n",
      " A\n",
      " sav\n",
      "ory\n",
      " cheese\n",
      " from\n",
      " the\n",
      " Al\n",
      "ps\n",
      " with\n",
      " a\n",
      " nut\n",
      "ty\n",
      " taste\n",
      " and\n",
      " a\n",
      " soft\n",
      ",\n",
      " washed\n",
      " r\n",
      "ind\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      ".\n",
      " **\n",
      "Hard\n",
      " Che\n",
      "es\n",
      "es\n",
      ":**\n",
      "\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Com\n",
      "té\n",
      "**:\n",
      " A\n",
      " firm\n",
      ",\n",
      " nut\n",
      "ty\n",
      " cheese\n",
      " made\n",
      " from\n",
      " un\n",
      "p\n",
      "aste\n",
      "ur\n",
      "ized\n",
      " cow\n",
      "'\n",
      "s\n",
      " milk\n",
      ",\n",
      " often\n",
      " aged\n",
      " for\n",
      " extended\n",
      " periods\n",
      ".\n",
      "\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Be\n",
      "au\n",
      "fort\n",
      "**:\n",
      " Similar\n",
      " to\n",
      " Gru\n",
      "y\n",
      "ère\n",
      ",\n",
      " this\n",
      " cheese\n",
      " has\n",
      " a\n",
      " firm\n",
      " texture\n",
      " and\n",
      " a\n",
      " sweet\n",
      ",\n",
      " nut\n",
      "ty\n",
      " flavor\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      ".\n",
      " **\n",
      "Blue\n",
      " Che\n",
      "es\n",
      "es\n",
      ":**\n",
      "\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Ro\n",
      "qu\n",
      "ef\n",
      "ort\n",
      "**:\n",
      " A\n",
      " strong\n",
      ",\n",
      " tang\n",
      "y\n",
      " sheep\n",
      " milk\n",
      " cheese\n",
      " with\n",
      " distinctive\n",
      " blue\n",
      " ve\n",
      "ins\n",
      ".\n",
      "\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "B\n",
      "le\n",
      "u\n",
      " d\n",
      "'\n",
      "A\n",
      "u\n",
      "ver\n",
      "g\n",
      "ne\n",
      "**:\n",
      " A\n",
      " cream\n",
      "y\n",
      " and\n",
      " mild\n",
      "er\n",
      " blue\n",
      " cheese\n",
      " with\n",
      " a\n",
      " distinctive\n",
      " arom\n",
      "a\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      ".\n",
      " **\n",
      "Go\n",
      "at\n",
      " Che\n",
      "es\n",
      "es\n",
      ":**\n",
      "\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Ch\n",
      "è\n",
      "vre\n",
      "**:\n",
      " Available\n",
      " in\n",
      " various\n",
      " forms\n",
      ",\n",
      " from\n",
      " fresh\n",
      " and\n",
      " cream\n",
      "y\n",
      " to\n",
      " aged\n",
      " and\n",
      " cr\n",
      "umb\n",
      "ly\n",
      ",\n",
      " with\n",
      " a\n",
      " tang\n",
      "y\n",
      ",\n",
      " earth\n",
      "y\n",
      " flavor\n",
      ".\n",
      "\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "C\n",
      "rott\n",
      "in\n",
      " de\n",
      " Ch\n",
      "av\n",
      "ign\n",
      "ol\n",
      "**:\n",
      " A\n",
      " small\n",
      ",\n",
      " round\n",
      " go\n",
      "at\n",
      " cheese\n",
      " with\n",
      " a\n",
      " dense\n",
      " texture\n",
      " and\n",
      " a\n",
      " nut\n",
      "ty\n",
      ",\n",
      " slightly\n",
      " tart\n",
      " flavor\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "U\n",
      "lt\n",
      "imately\n",
      ",\n",
      " the\n",
      " \"\n",
      "best\n",
      "\"\n",
      " French\n",
      " cheese\n",
      " is\n",
      " the\n",
      " one\n",
      " that\n",
      " you\n",
      " enjoy\n",
      " the\n",
      " most\n",
      ".\n",
      " Exper\n",
      "iment\n",
      "ing\n",
      " with\n",
      " different\n",
      " types\n",
      " can\n",
      " help\n",
      " you\n",
      " discover\n",
      " your\n",
      " personal\n",
      " favorite\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "api_key = \"H1pJaiozF8BK9hkWc14qeSWCJll31t7U\"\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "stream_response = client.chat.stream(\n",
    "    model = model,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the best French cheese?\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "for chunk in stream_response:\n",
    "    print(chunk.data.choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting navigation...\n",
      "Path: 14th_century -> African_slave_trade\n",
      "Step 0: 14th_century\n",
      "Available links: 13th_century, 15th_century, Abacus, Aztec, Black_Death, Buddha, China, Christianity, Dante_Alighieri, Dark_Ages, Edward_III_of_England, England, English_peasants%27_revolt_of_1381, Europe, France, Hundred_Years%27_War, Ibn_Battuta, India, Islam, Italy, Lithuania, Ming_Dynasty, Niger, Ottoman_Empire, Poland, Pope, Renaissance, Singapore, Time, Timur, Washington%2C_D.C.\n",
      "Selected link: Ottoman_Empire\n",
      "Step 1: Ottoman_Empire\n",
      "Available links: 15th_century, 16th_century, 17th_century, 20th_century, Abbasid, Albania, Algeria, Arabic_language, Armenia, Asia, Atlantic_Ocean, Austria, Azerbaijan, Baghdad, Baroque, Beirut, Black_Sea, Bulgaria, Byzantine_Empire, Calligraphy, Caspian_Sea, Christopher_Columbus, Constitutional_monarchy, Crimean_War, Currency, Cyprus, Egypt, England, Europe, Folk_music, France, Greece, Greek_War_of_Independence, Guild, Habsburg_Spain, Holy_Roman_Empire, Indian_Ocean, Islam, Istanbul, Jerusalem, Johannes_Gutenberg, Libya, List_of_countries_by_system_of_government, Marco_Polo, Mediterranean_Sea, Mesopotamia, Middle_Ages, Middle_East, Moldova, Monarchy, Mongol_Empire, Montenegro, Muhammad, Musical_instrument, Nationalism, North_Africa, Persian_Gulf, Piano, Poland, Portugal, Qur%27an, Red_Sea, Rococo, Roman_Empire, Romania, Russia, Russian_Revolution_of_1917, Sarajevo, Serbia, Siege, Slavery, Slovakia, Spanish_Inquisition, Sudan, Suleiman_the_Magnificent, Sultan, The_Holocaust, Tunis, Turkey, Ukraine, University, Vienna, World_War_I, Yemen\n",
      "Selected link: Slavery\n",
      "Step 2: Slavery\n",
      "Available links: 17th_century, 2nd_century, Africa, Agriculture, American_Civil_War, Anarchism, Ancient_Egypt, Ancient_Greece, Ancient_Rome, Angola, Animal_rights, Arabic_language, Asia, Atlantic_slave_trade, Augustine_of_Hippo, Aztec, Babylonia, Bible, Brazil, British_Empire, Canada, China, Cholera, Claudius, Cocoa, Code_of_Hammurabi, Colombia, Communism, Cotton, Cuba, East_Africa, Ecuador, Egypt, English_language, Europe, European_Union, Famine, France, Frederick_Douglass, Ghana, Goa, Greece, Guadeloupe, Guinea, HIV, Haiti, Hinduism, Homer, India, Indian_Ocean, Industry, Iraq, Irish_people, Israel, Istanbul, Latin_America, League_of_Nations, Liberia, London, Mali, Martinique, Maya_civilization, Mesopotamia, Mining, Monarchy, Muhammad, Napoleon_I_of_France, Nationality, Netherlands, New_Zealand, North_America, Ottoman_Empire, Panama, Peru, Plato, Portugal, Poverty, Property, Puerto_Rico, Qur%27an, Race, Ramesses_II, Red_Sea, Religion, Roman_Empire, Russia, Senegal, Sierra_Leone, Smallpox, Socialism, South_America, Spain, Sugar, Textile, Thailand, Tobacco, Trade_union, Turkey, United_Kingdom, United_States, Venezuela, Viking\n",
      "Selected link: Atlantic_slave_trade\n",
      "Step 3: Atlantic_slave_trade\n",
      "Available links: African_American_literature, African_slave_trade, Angola, Austria, Aztec, Barbados, Benin, Birmingham, Brazil, Bristol, Cameroon, Continent, Cotton, Denmark, England, Equatorial_Guinea, France, French_Revolution, Gabon, Germany, Ghana, Great_Britain, Guadeloupe, Guinea, Guinea-Bissau, Haiti, Hern%C3%A1n_Cort%C3%A9s, Industrial_Revolution, Jamaica, Liberia, Liverpool, Madagascar, Maya_civilization, Mexico, Mining, Minstrel_show, Mozambique, Napoleonic_Wars, Netherlands, Nigeria, Ottoman_Empire, Portugal, Religious_Society_of_Friends, Royal_Navy, Russia, Scotland, Senegal, Sierra_Leone, Slavery, South_Africa, Spain, Sugar, Sweden, Tobacco, Togo, United_Kingdom, United_States, William_Wilberforce\n",
      "Selected link: African_slave_trade\n",
      "Navigation completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "api_key = \"H1pJaiozF8BK9hkWc14qeSWCJll31t7U\"\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "DATA_FOLDER = 'data/wikispeedia_paths-and-graph/'\n",
    "\n",
    "# Read the data files\n",
    "links = pd.read_csv(DATA_FOLDER + 'links.tsv', sep='\\t', skiprows=11, names=['linkSource', 'linkTarget'])\n",
    "paths_finished = pd.read_csv(DATA_FOLDER + 'paths_finished.tsv', sep='\\t', skiprows=15, names=['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating'])\n",
    "\n",
    "# Prepare the links dictionary for fast lookup\n",
    "links_dict = links.groupby('linkSource')['linkTarget'].apply(list).to_dict()\n",
    "\n",
    "# Initialize output data structures\n",
    "llm_choices = []\n",
    "llm_paths = []\n",
    "\n",
    "run_id = 2  # Unique identifier for the run\n",
    "\n",
    "print(\"Starting navigation...\")\n",
    "\n",
    "# Iterate over just one path in paths_finished.tsv for testing\n",
    "for index, row in paths_finished.head(1).iterrows():\n",
    "    path = row['path'].split(';')\n",
    "    start_article = path[0]\n",
    "    end_article = path[-1]\n",
    "\n",
    "    current_article = start_article\n",
    "    steps = 0\n",
    "    path_taken = [current_article]\n",
    "\n",
    "    print(f\"Path: {start_article} -> {end_article}\")\n",
    "\n",
    "    while current_article != end_article:\n",
    "        # Retrieve the links of the current article\n",
    "        linked_articles = links_dict.get(current_article, [])\n",
    "\n",
    "        if not linked_articles:\n",
    "            print(f\"No outgoing links from {current_article}.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Step {steps}: {current_article}\")\n",
    "        print(f\"Available links: {', '.join(linked_articles)}\")\n",
    "\n",
    "        # Prepare the prompt for the LLM\n",
    "        prompt = f\"You are navigating Wikipedia from '{start_article}' to '{end_article}'.\\n\" \\\n",
    "                 f\"Currently at '{current_article}'.\\n\" \\\n",
    "                 f\"Available links: {', '.join(linked_articles)}.\\n\" \\\n",
    "                 f\"Which article would you like to visit next? Respond only with the article name.\"\n",
    "\n",
    "        # Call the OpenAI API\n",
    "        chat_completion = client.chat.complete(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=10,\n",
    "            temperature=0,\n",
    "            n=1\n",
    "        )\n",
    "\n",
    "        # Extract the LLM's choice\n",
    "        choice = chat_completion.choices[0].message.content\n",
    "\n",
    "        # Validate the choice\n",
    "        if choice not in linked_articles:\n",
    "            print(f\"Invalid choice '{choice}' made by the LLM. Selecting the first link as fallback.\")\n",
    "            choice = linked_articles[0]\n",
    "\n",
    "        # Record the choice\n",
    "        llm_choices.append({\n",
    "            'run_id': run_id,\n",
    "            'article': current_article,\n",
    "            'links': linked_articles,\n",
    "            'link_chosen': choice\n",
    "        })\n",
    "\n",
    "        print(f\"Selected link: {choice}\")\n",
    "\n",
    "        # Update the path and step\n",
    "        current_article = choice\n",
    "        path_taken.append(current_article)\n",
    "        steps += 1\n",
    "\n",
    "        # To comply with OpenAI rate limits\n",
    "        time.sleep(10)\n",
    "\n",
    "    # Record the path taken\n",
    "    llm_paths.append({\n",
    "        'run_id': run_id,\n",
    "        'steps': steps,\n",
    "        'path': ';'.join(path_taken)\n",
    "    })\n",
    "\n",
    "# Convert the results to DataFrames\n",
    "llm_choices_df = pd.DataFrame(llm_choices)\n",
    "llm_paths_df = pd.DataFrame(llm_paths)\n",
    "\n",
    "# Save the results to TSV files\n",
    "llm_choices_df.to_csv('tests/llm_choices.tsv', sep='\\t', index=False)\n",
    "llm_paths_df.to_csv('tests/llm_paths.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(\"Navigation completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
